{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0804608-b4fb-4732-8011-8821d2948df4",
   "metadata": {},
   "source": [
    "![](http://uwe-cyber.github.io/images/uwe_banner.png)\n",
    "\n",
    "# Conversational Agent - aka ChatBot\n",
    "\n",
    "In this example, we develop a interactive notebook to have conversation with a large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56617a2f-e2a6-435b-8a4b-5b00a7d64291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt4all-13b-snoozy-q4_0.gguf', 'mistral-7b-openorca.Q4_0.gguf', 'wizardlm-13b-v1.2.Q4_0.gguf', 'nous-hermes-llama2-13b.Q4_0.gguf', 'gpt4all-falcon-q4_0.gguf', 'orca-mini-3b-gguf2-q4_0.gguf']\n",
      "llama_new_context_with_model: max tensor size =   103.76 MB\n",
      "Using model:  orca-mini-3b-gguf2-q4_0.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: using Vulkan on NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "#GPU usage\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "model_names = ['gpt4all-13b-snoozy-q4_0.gguf', 'mistral-7b-openorca.Q4_0.gguf', 'wizardlm-13b-v1.2.Q4_0.gguf', 'nous-hermes-llama2-13b.Q4_0.gguf', 'gpt4all-falcon-q4_0.gguf', 'orca-mini-3b-gguf2-q4_0.gguf']\n",
    "print(model_names)\n",
    "name = model_names[5]\n",
    "\n",
    "model = GPT4All(name, device='gpu')\n",
    "print ('Using model: ', name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0072c6-fa99-4cf6-8c27-218f41b66586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Q:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  irl, sorry for the interruption. Can you please tell me what you're doing?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Q:  I'm writing some code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  ! to use in your code.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Q:  !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:   Omaha, Nebraska is located in the Midwestern region of the United States. It is known for its strong economy and diverse population. What are some notable landmarks or attractions in Omaha?\n"
     ]
    }
   ],
   "source": [
    "input_text = ''\n",
    "while True:\n",
    "    input_text = input(\"Q: \")\n",
    "    #print (\"Q: \", input_text)\n",
    "    if input_text == 'exit':\n",
    "        response = 'Bye for now.'\n",
    "        print (\"A: \", response)\n",
    "        break\n",
    "    prompt = '### Human: ' + input_text + '\\n### Assistant: '\n",
    "    response = model.generate(prompt, max_tokens=4096, temp=0.7, top_k=40, top_p=0.4, repeat_penalty=1.18, repeat_last_n=64)\n",
    "    print (\"A: \", response)\n",
    "    # do something with input text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
